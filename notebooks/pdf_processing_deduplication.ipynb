{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0106ba2f",
   "metadata": {},
   "source": [
    "### 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c187f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4ad28",
   "metadata": {},
   "source": [
    "- hashlib: Provides hash functions (like SHA-256) to create unique digital fingerprints of files or data.\n",
    "\n",
    "- json: Allows reading/writing JSON data (a text-based format to store structured data).\n",
    "\n",
    "- os: Provides utilities to interact with the operating system (like checking if files exist).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b75318",
   "metadata": {},
   "source": [
    "### 2. Setting constant filename for tracking processed PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce356be",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_RECORD_FILE = \"processed_pdfs.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0310f",
   "metadata": {},
   "source": [
    "- This is a constant variable holding the filename where you will store the list of PDFs that have been processed.\n",
    "\n",
    "- The file will store hashes (unique IDs) of PDFs processed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebe812",
   "metadata": {},
   "source": [
    "### 3. Function: Generate a hash for a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b691fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hash(file_path):\n",
    "    \"\"\"Generate a hash for the file content.\"\"\"\n",
    "    hasher = hashlib.sha256()                          # Create a new SHA-256 hash object\n",
    "    with open(file_path, \"rb\") as f:                   # Open the file in binary read mode\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):  # Read file in 4KB chunks until EOF (empty bytes)\n",
    "            hasher.update(chunk)                       # Update hash with current chunk data\n",
    "    return hasher.hexdigest()                          # Return the final hex digest (unique hash string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed32e47",
   "metadata": {},
   "source": [
    "- Purpose: Calculate a unique fingerprint of the file content to detect duplicates.\n",
    "\n",
    "- Opens the file as binary because hashing works on raw bytes.\n",
    "\n",
    "- Reads file in chunks (4 KB at a time) for memory efficiency — useful for big files.\n",
    "\n",
    "- Updates the hash object incrementally for each chunk.\n",
    "\n",
    "- Returns the final hexadecimal string representation of the hash — a 64-character unique string for that file content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79c2f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash for ./test_data/Passport1.pdf: c2cfca0247c8cce1e59a62bb32598b98f2117f443ca6d2897338422be16ddad4\n"
     ]
    }
   ],
   "source": [
    "# lets try with a sample file\n",
    "sample_file_path = \"./test_data/Passport1.pdf\"\n",
    "sample_file_hash = get_file_hash(sample_file_path)\n",
    "print(f\"Hash for {sample_file_path}: {sample_file_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04aaadd",
   "metadata": {},
   "source": [
    "### 4. Function: Load processed PDF hashes from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d21d27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_records():\n",
    "    if os.path.exists(PROCESSED_RECORD_FILE):        # Check if JSON file exists\n",
    "        with open(PROCESSED_RECORD_FILE, \"r\") as f:  # Open JSON file in read mode\n",
    "            return set(json.load(f))                 # Load list from JSON, convert to set for faster lookup\n",
    "    return set()                                     # If file doesn't exist, return empty set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e738b4",
   "metadata": {},
   "source": [
    "- Checks if processed_pdfs.json file exists.\n",
    "\n",
    "- If yes, loads the list of processed file hashes from it.\n",
    "\n",
    "- Converts list to a Python set (fast lookup for hash presence).\n",
    "\n",
    "- If the file does not exist, return an empty set (means no PDFs processed yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a0067",
   "metadata": {},
   "source": [
    "### 5. Function: Save updated processed PDF hashes back to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d56c05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_records(processed_set):\n",
    "    with open(PROCESSED_RECORD_FILE, \"w\") as f:    # Open JSON file in write mode (overwrites existing)\n",
    "        json.dump(list(processed_set), f)          # Convert set to list and write as JSON array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1dcd1",
   "metadata": {},
   "source": [
    "- Writes the updated set of processed hashes back to the JSON file.\n",
    "\n",
    "- Converts set back to list because JSON doesn’t support sets.\n",
    "\n",
    "- Overwrites the file each time to keep it updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a80e83",
   "metadata": {},
   "source": [
    "### 6. Main function: Process a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66a80e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.pdf_loader import load_pdf\n",
    "from modules.text_splitter import split_text\n",
    "from modules.embed_store import embed_and_store_documents\n",
    "from modules.qa_with_retriever import answer_query_with_gemini\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    processed = load_processed_records()        # Load processed hashes so far\n",
    "    pdf_hash = get_file_hash(pdf_path)          # Calculate hash for current PDF\n",
    "\n",
    "    if pdf_hash in processed:                    # Check if this PDF was already processed\n",
    "        print(\"PDF already processed, skipping.\")\n",
    "        return                                  # Exit function early if processed\n",
    "\n",
    "    # If not processed:\n",
    "    pdf_text = load_pdf(pdf_path)                # Load the text content from PDF\n",
    "    chunks = split_text(pdf_text, chunk_size=1000, chunk_overlap=100)  # Split into chunks for RAG\n",
    "    vectorstore = embed_and_store_documents(chunks, persist_directory=\"./chroma_store\", model_name=\"embed-multilingual-v3.0\")  # Embed & store\n",
    "\n",
    "    processed.add(pdf_hash)                       # Add this PDF’s hash to the processed set\n",
    "    save_processed_records(processed)             # Save updated processed hashes back to JSON\n",
    "    print(\"PDF processed and embeddings stored successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb413c2",
   "metadata": {},
   "source": [
    "- Loads all processed PDF hashes to check for duplicates.\n",
    "\n",
    "- Gets the hash of the current PDF file.\n",
    "\n",
    "- If hash already in processed set → skips further processing.\n",
    "\n",
    "- If not processed:\n",
    "\n",
    "    Reads and extracts text from the PDF (you must define load_pdf).\n",
    "\n",
    "    Splits the text into chunks with specified size and overlap (you must define split_text).\n",
    "\n",
    "    Embeds chunks and stores them persistently in a vector store (you must define embed_and_store_documents).\n",
    "\n",
    "- Adds the current PDF hash to processed set.\n",
    "\n",
    "- Saves the updated processed set back to JSON file for future reference.\n",
    "\n",
    "- Prints success message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5929bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.pdf_loader import load_pdf\n",
    "from modules.text_splitter import split_text\n",
    "from modules.embed_store import embed_and_store_documents\n",
    "from modules.qa_with_retriever import answer_query_with_gemini\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    processed = load_processed_records()        # Load processed hashes so far\n",
    "    pdf_hash = get_file_hash(pdf_path)          # Calculate hash for current PDF\n",
    "\n",
    "    if pdf_hash in processed:                    # Check if this PDF was already processed\n",
    "        print(\"PDF already processed, skipping.\")\n",
    "        return                                  # Exit function early if processed\n",
    "\n",
    "    # If not processed:\n",
    "    pdf_text = load_pdf(pdf_path)                # Load the text content from PDF\n",
    "    chunks = split_text(pdf_text, chunk_size=1000, chunk_overlap=100)  # Split into chunks for RAG\n",
    "    vectorstore = embed_and_store_documents(chunks, persist_directory=\"./chroma_store\", model_name=\"embed-multilingual-v3.0\")  # Embed & store\n",
    "\n",
    "    processed.add(pdf_hash)                       # Add this PDF’s hash to the processed set\n",
    "    save_processed_records(processed)             # Save updated processed hashes back to JSON\n",
    "    print(\"PDF processed and embeddings stored successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75000c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
