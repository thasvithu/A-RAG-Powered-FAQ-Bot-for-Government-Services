{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab14ff9b",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:black; color:lime; padding:10px; font-family:monospace;\">\n",
    "üìú pdf_loader.py\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867763e",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf(file_path: str):\n",
    "    \"\"\"\n",
    "    Load a PDF file and return its content as a list of documents.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of documents extracted from the PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    return documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Explanation**\n",
    "\n",
    "### **1. Import**\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "```\n",
    "\n",
    "* **`PyPDFLoader`** is a special class in LangChain‚Äôs **community** package.\n",
    "* It‚Äôs used for **reading PDF files** and converting them into a format that LangChain understands ‚Äî namely, a **list of Document objects**.\n",
    "* Each **Document** contains:\n",
    "\n",
    "  * `.page_content` ‚Üí the text of the page.\n",
    "  * `.metadata` ‚Üí extra info like page number, file name, etc.\n",
    "\n",
    "Think of this as a **PDF-to-structured-text converter** that‚Äôs LangChain-friendly.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Function Definition**\n",
    "\n",
    "```python\n",
    "def load_pdf(file_path: str):\n",
    "```\n",
    "\n",
    "* `file_path` ‚Üí the location of your PDF file as a **string**.\n",
    "* The `: str` after `file_path` is **type hinting**, telling other developers (and IDEs) that this parameter should be a string.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Loader Creation**\n",
    "\n",
    "```python\n",
    "loader = PyPDFLoader(file_path)\n",
    "```\n",
    "\n",
    "* Here you‚Äôre **creating an instance** of `PyPDFLoader` and telling it which PDF file to read.\n",
    "* At this point, the file isn‚Äôt fully processed yet ‚Äî you‚Äôve just prepared a ‚Äúloader‚Äù object that knows **where to look**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Load the PDF**\n",
    "\n",
    "```python\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "* `.load()` actually **opens the PDF, reads each page, and extracts the text**.\n",
    "* It splits the PDF into **a list of `Document` objects** ‚Äî usually one per page.\n",
    "* Example of what `documents` might look like:\n",
    "\n",
    "```python\n",
    "[\n",
    "    Document(page_content=\"Text from page 1\", metadata={\"page\": 1, \"source\": \"file.pdf\"}),\n",
    "    Document(page_content=\"Text from page 2\", metadata={\"page\": 2, \"source\": \"file.pdf\"})\n",
    "]\n",
    "```\n",
    "\n",
    "* This structure is perfect for **LLM pipelines** because:\n",
    "\n",
    "  1. You can process each page separately.\n",
    "  2. Metadata helps you know where text came from.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Return the Documents**\n",
    "\n",
    "```python\n",
    "return documents\n",
    "```\n",
    "\n",
    "* The function sends back the **list of Document objects** so you can use them later for:\n",
    "\n",
    "  * Splitting into smaller chunks\n",
    "  * Creating embeddings\n",
    "  * Storing in a vector database\n",
    "  * Running LLM queries\n",
    "\n",
    "---\n",
    "\n",
    "## **Why This Function is Useful**\n",
    "\n",
    "* You now have a **reusable utility**: just call `load_pdf(\"myfile.pdf\")` and instantly get all text in LangChain‚Äôs Document format.\n",
    "* It hides the internal complexity of opening, parsing, and structuring the PDF.\n",
    "* You can later swap `PyPDFLoader` for another loader (e.g., `PyMuPDFLoader`) without changing the rest of your code.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In short**:\n",
    "This function **loads a PDF**, processes it into a **list of page-level Document objects**, and returns them ‚Äî making it the first step in most **RAG (Retrieval-Augmented Generation)** pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa6e97",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:black; color:lime; padding:10px; font-family:monospace;\">\n",
    "üìú text_splitter.py\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222210a",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def split_text(documents: List[Document], chunk_size=512, chunk_overlap=64):\n",
    "    \"\"\"\n",
    "    Split a list of LangChain Document objects into smaller chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): The documents to split.\n",
    "        chunk_size (int): Max size of each chunk.\n",
    "        chunk_overlap (int): Overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: Smaller chunks with metadata preserved.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents) \n",
    "    \n",
    "    return chunks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ What‚Äôs the purpose of this function?**\n",
    "\n",
    "When working with **Large Language Models (LLMs)**, we can‚Äôt just give them huge documents (like an entire PDF at once) ‚Äî they have **context limits** (e.g., 4,000 tokens).\n",
    "So, we **split big documents into smaller chunks** while still preserving meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Imports explained**\n",
    "\n",
    "* **`RecursiveCharacterTextSplitter`**\n",
    "\n",
    "  * A tool from LangChain that splits text into smaller chunks **smartly**, without cutting in the middle of words or breaking meaning unnecessarily.\n",
    "  * It tries larger separators (`\\n\\n` = paragraph) first, then smaller ones (`\\n`, `.` = sentence, `\" \"` = space), and finally splits at the character level if needed.\n",
    "\n",
    "* **`List`** (from `typing`)\n",
    "\n",
    "  * Used for type hints to specify that `documents` is a **list** of something.\n",
    "\n",
    "* **`Document`** (from `langchain_core.documents`)\n",
    "\n",
    "  * LangChain‚Äôs special object that holds both:\n",
    "\n",
    "    1. **Text content** (`page_content`)\n",
    "    2. **Metadata** (like page number, source filename, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Function Parameters**\n",
    "\n",
    "* **`documents`**: A list of LangChain `Document` objects (already loaded from a PDF or other source).\n",
    "* **`chunk_size`**:\n",
    "\n",
    "  * Maximum number of **characters** in each chunk.\n",
    "  * Default: `512`.\n",
    "* **`chunk_overlap`**:\n",
    "\n",
    "  * Number of characters to **repeat** between chunks.\n",
    "  * This ensures continuity so that important sentences spanning two chunks aren‚Äôt broken.\n",
    "  * Default: `64`.\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ How the splitter works**\n",
    "\n",
    "```python\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "```\n",
    "\n",
    "* **`separators`**: Order in which the splitter tries to break text:\n",
    "\n",
    "  1. Paragraphs (`\\n\\n`)\n",
    "  2. New lines (`\\n`)\n",
    "  3. Sentences (`.`)\n",
    "  4. Spaces (` `)\n",
    "  5. No separator (character-by-character as a last resort)\n",
    "\n",
    "The splitter tries to **preserve natural breaks** first, and only if that fails, it cuts smaller.\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ Splitting the documents**\n",
    "\n",
    "```python\n",
    "chunks = splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "* `split_documents`:\n",
    "\n",
    "  * Takes a list of `Document` objects\n",
    "  * Splits them into **multiple smaller `Document` objects**\n",
    "  * **Preserves metadata** (e.g., if the original had `{\"source\": \"file.pdf\"}`, each chunk will still have that metadata)\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ Return value**\n",
    "\n",
    "* **Returns**: A **list of small chunks**, each a `Document` object containing:\n",
    "\n",
    "  * `page_content` ‚Üí the chunk‚Äôs text\n",
    "  * `metadata` ‚Üí same metadata as the original\n",
    "\n",
    "---\n",
    "\n",
    "## **7Ô∏è‚É£ Example**\n",
    "\n",
    "If you have **one PDF page** with 1,500 characters:\n",
    "\n",
    "* `chunk_size=512`, `chunk_overlap=64` ‚Üí It might split into:\n",
    "\n",
    "  * **Chunk 1**: characters 0‚Äì512\n",
    "  * **Chunk 2**: characters 448‚Äì960 (64 characters overlap with Chunk 1)\n",
    "  * **Chunk 3**: characters 896‚Äìend\n",
    "\n",
    "This overlap ensures sentences are not lost when chunks are processed separately.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In short:**\n",
    "This function takes large text documents and **cuts them into bite-sized pieces** that are small enough for LLMs to handle, **while preserving important context** between pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce08943",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:black; color:lime; padding:10px; font-family:monospace;\">\n",
    "üìú embed_store.py\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa899e",
   "metadata": {},
   "source": [
    "## **Big Picture**\n",
    "\n",
    "This function takes a list of **documents** (already loaded, e.g., from PDFs or text files), converts them into **embeddings** using Cohere‚Äôs multilingual model, and stores them inside a **Chroma vector database** on disk for future retrieval.\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "üìÑ Documents ‚Üí üß† Convert to numbers (embeddings) ‚Üí üì¶ Store in Chroma DB ‚Üí üîç Search later.\n",
    "\n",
    "---\n",
    "\n",
    "## **Detailed Breakdown**\n",
    "\n",
    "### 1Ô∏è‚É£ Imports\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "```\n",
    "\n",
    "* **Chroma** ‚Üí a **vector store** that saves and retrieves embeddings efficiently.\n",
    "* You can save it to disk and reload later.\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import CohereEmbeddings\n",
    "```\n",
    "\n",
    "* **CohereEmbeddings** ‚Üí LangChain wrapper for Cohere‚Äôs embedding API.\n",
    "* We‚Äôll use it to convert text into **vector representations**.\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "```\n",
    "\n",
    "* **Document** ‚Üí LangChain‚Äôs standard way to represent a text chunk + metadata.\n",
    "* **List** ‚Üí For type hinting, saying `documents` should be a **list** of `Document`.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "```\n",
    "\n",
    "* `load_dotenv()` loads **environment variables** from a `.env` file (e.g., your API key).\n",
    "* `os.getenv()` will then fetch `COHERE_API_KEY` securely.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Load environment variables\n",
    "\n",
    "```python\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "* Reads your `.env` file.\n",
    "* Ensures `os.getenv(\"COHERE_API_KEY\")` works.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ The Function\n",
    "\n",
    "```python\n",
    "def embed_and_store_documents(\n",
    "    documents: List[Document],\n",
    "    persist_directory: str = \"./chroma_store\",\n",
    "    model_name: str = \"embed-multilingual-v3.0\"\n",
    "):\n",
    "```\n",
    "\n",
    "* **documents** ‚Üí The list of already-loaded text chunks.\n",
    "* **persist\\_directory** ‚Üí Folder where Chroma will store the embeddings on disk.\n",
    "* **model\\_name** ‚Üí Which Cohere model to use. Default is multilingual (`embed-multilingual-v3.0`).\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Create the Embedding Model\n",
    "\n",
    "```python\n",
    "embedding_model = CohereEmbeddings(\n",
    "    cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "    model=model_name,\n",
    "    user_agent=\"langchain\" \n",
    ")\n",
    "```\n",
    "\n",
    "* Uses your **Cohere API key**.\n",
    "* Uses the specified **model**.\n",
    "* `user_agent=\"langchain\"` is just an identifier for API usage tracking.\n",
    "\n",
    "üí° **What it does:**\n",
    "This object will take any text and return its **vector embedding** (a list of numbers).\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Create / Load the Vector Store\n",
    "\n",
    "```python\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "```\n",
    "\n",
    "* **persist\\_directory** ‚Üí Where the Chroma DB will be saved so you can reload later.\n",
    "* **embedding\\_function** ‚Üí The embedding model to use for converting queries & documents.\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ Add Documents to the Vector Store\n",
    "\n",
    "```python\n",
    "vectorstore.add_documents(documents)\n",
    "```\n",
    "\n",
    "* Converts each `Document` into **embeddings** using the `embedding_model`.\n",
    "* Saves them in the **Chroma DB**.\n",
    "* This makes them **searchable by meaning**, not just keywords.\n",
    "\n",
    "---\n",
    "\n",
    "### 7Ô∏è‚É£ Confirmation\n",
    "\n",
    "```python\n",
    "print(f\"‚úÖ Stored {len(documents)} documents.\")\n",
    "```\n",
    "\n",
    "* Prints how many documents were stored.\n",
    "\n",
    "---\n",
    "\n",
    "### 8Ô∏è‚É£ Return the Vector Store\n",
    "\n",
    "```python\n",
    "return vectorstore\n",
    "```\n",
    "\n",
    "* Lets you use the `vectorstore` immediately for retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## **Flow Summary**\n",
    "\n",
    "1. Load your `.env` so API keys are available.\n",
    "2. Create a Cohere embedding model.\n",
    "3. Create / load a Chroma vector database.\n",
    "4. Convert all your documents into embeddings.\n",
    "5. Store them in the Chroma DB.\n",
    "6. Return the DB object for querying later.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Example Usage**\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"This is a sample text about AI.\"),\n",
    "    Document(page_content=\"Another document about machine learning.\")\n",
    "]\n",
    "\n",
    "vectorstore = embed_and_store_documents(docs)\n",
    "\n",
    "# Later, you can search:\n",
    "results = vectorstore.similarity_search(\"What is AI?\")\n",
    "for r in results:\n",
    "    print(r.page_content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506fd7d",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:black; color:lime; padding:10px; font-family:monospace;\">\n",
    "üìú retriver.py\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70a84c",
   "metadata": {},
   "source": [
    "## **Code Purpose**\n",
    "\n",
    "This function `get_retriever` sets up a **Retriever** using:\n",
    "\n",
    "* **Cohere embeddings** for text vectorization (turning text into number vectors).\n",
    "* **Chroma** as the **vector database** to store and search those vectors.\n",
    "* Returns a retriever that can find the top-5 most similar documents for a given query.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Breakdown**\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "```\n",
    "\n",
    "* **Imports the Chroma vector database integration** for LangChain.\n",
    "* Chroma stores your documents in a vectorized form and allows fast similarity search.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import CohereEmbeddings\n",
    "```\n",
    "\n",
    "* Imports **CohereEmbeddings** class from LangChain's community module.\n",
    "* Cohere provides embedding models like `\"embed-multilingual-v3.0\"` that support multiple languages.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import os\n",
    "```\n",
    "\n",
    "* Imports Python‚Äôs built-in `os` module to access environment variables (for the API key).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def get_retriever(persist_directory=\"./chroma_store\", model_name=\"embed-multilingual-v3.0\"):\n",
    "```\n",
    "\n",
    "* Defines a **function** `get_retriever` that:\n",
    "\n",
    "  * `persist_directory` ‚Üí Folder where Chroma stores its data files.\n",
    "  * `model_name` ‚Üí Name of the Cohere embedding model (default: multilingual model).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    embedding = CohereEmbeddings(\n",
    "        cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "        model=model_name,\n",
    "        user_agent=\"langchain\"\n",
    "    )\n",
    "```\n",
    "\n",
    "* Creates an **embedding object** that:\n",
    "\n",
    "  * Reads the Cohere API key from an **environment variable** (`COHERE_API_KEY`).\n",
    "  * Uses the specified `model_name` to generate embeddings.\n",
    "  * Sets `user_agent=\"langchain\"` so Cohere knows this request is from a LangChain app.\n",
    "\n",
    "üí° **What embeddings do**: They convert text into numerical vectors so we can measure \"semantic similarity\" between pieces of text.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    vectorstore = Chroma(   \n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "```\n",
    "\n",
    "* Creates a **Chroma vector database** object that:\n",
    "\n",
    "  * Uses `persist_directory` for saving/reloading the database.\n",
    "  * Uses the `embedding_function` (Cohere) to embed new data when needed.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    return vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "```\n",
    "\n",
    "* Turns the vector store into a **Retriever**.\n",
    "* **`search_type=\"similarity\"`** ‚Üí finds documents whose embeddings are closest to the query embedding.\n",
    "* **`search_kwargs={\"k\": 5}`** ‚Üí returns the **top 5 most similar** documents.\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works Together**\n",
    "\n",
    "1. **Embedding Model** ‚Äî Cohere turns your query & documents into vectors.\n",
    "2. **Vector Store** ‚Äî Chroma stores those vectors in a database on disk (`./chroma_store`).\n",
    "3. **Retriever** ‚Äî Instead of you writing the search logic, `.as_retriever()` returns a ready-to-use search tool.\n",
    "4. **Result** ‚Äî When you pass a query to the retriever, it:\n",
    "\n",
    "   * Embeds the query with Cohere.\n",
    "   * Compares it with stored vectors.\n",
    "   * Returns the top-5 most relevant document chunks.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Example Usage**\n",
    "\n",
    "```python\n",
    "retriever = get_retriever()\n",
    "\n",
    "# Search for relevant documents\n",
    "docs = retriever.get_relevant_documents(\"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Øä‡Æ¥‡Æø ‡Æµ‡Æ∞‡Æ≤‡Ææ‡Æ±‡ØÅ\")  # Tamil language history\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51304d24",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:black; color:lime; padding:10px; font-family:monospace;\">\n",
    "üìú qa_with_retriever.py\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff2a58",
   "metadata": {},
   "source": [
    "## **Purpose of This Code**\n",
    "\n",
    "This script connects **Google‚Äôs Gemini AI model** with a **Retriever** (powered by Chroma + Cohere embeddings) to build a **multilingual Q\\&A bot for Sri Lankan government services**.\n",
    "\n",
    "When a user asks a question:\n",
    "\n",
    "1. The retriever searches for relevant document chunks from stored data.\n",
    "2. The Gemini model generates an answer in **the same language** as the question, using only the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Breakdown**\n",
    "\n",
    "### **Imports**\n",
    "\n",
    "```python\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from modules.retriever import get_retriever\n",
    "```\n",
    "\n",
    "* **`os`** ‚Äî Access environment variables (API keys).\n",
    "* **`google.generativeai`** ‚Äî Google‚Äôs SDK for Gemini AI models.\n",
    "* **`dotenv`** ‚Äî Loads `.env` file so API keys can be stored securely.\n",
    "* **`get_retriever`** ‚Äî Custom function (from your earlier code) that sets up a Chroma-based retriever for searching stored documents.\n",
    "\n",
    "---\n",
    "\n",
    "### **Load Environment Variables**\n",
    "\n",
    "```python\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "* Reads the `.env` file and loads values into environment variables.\n",
    "* This is where your **`GOOGLE_API_KEY`** is stored.\n",
    "\n",
    "---\n",
    "\n",
    "### **Configure Gemini API**\n",
    "\n",
    "```python\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "```\n",
    "\n",
    "* **`genai.configure`** ‚Üí Passes your Gemini API key so Google can authenticate your requests.\n",
    "* **`GenerativeModel(\"gemini-1.5-flash\")`** ‚Üí Loads the **Gemini 1.5 Flash** model (fast, multi-modal, and supports multiple languages).\n",
    "\n",
    "---\n",
    "\n",
    "### **Set Up the Retriever**\n",
    "\n",
    "```python\n",
    "retriever = get_retriever(\n",
    "    persist_directory=\"./chroma_store\",\n",
    "    model_name=\"embed-multilingual-v3.0\"\n",
    ")\n",
    "```\n",
    "\n",
    "* Creates a **Retriever**:\n",
    "\n",
    "  * Uses the multilingual Cohere embedding model to handle **Tamil, Sinhala, and English**.\n",
    "  * Reads from your **Chroma vector database** stored in `./chroma_store`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Main Function**\n",
    "\n",
    "```python\n",
    "def answer_query_with_gemini(query: str) -> str:\n",
    "```\n",
    "\n",
    "* Takes the **user‚Äôs question** (`query`) and returns an AI-generated answer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Retrieve Relevant Context**\n",
    "\n",
    "```python\n",
    "docs = retriever.invoke(query)\n",
    "if not docs:\n",
    "    return \"No relevant documents found for your question.\"\n",
    "```\n",
    "\n",
    "* **`retriever.invoke(query)`** ‚Üí Finds the top 5 most semantically similar document chunks to the query.\n",
    "* If no results ‚Üí returns a message saying nothing relevant was found.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "```\n",
    "\n",
    "* Joins all retrieved document texts into a **single block of context**.\n",
    "* This context will be fed into Gemini so it only answers from relevant official information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Prepare the Prompt for Gemini**\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "You are a trusted virtual assistant for Sri Lankan government services.\n",
    "\n",
    "Your job is to help users in **Tamil**, **Sinhala**, or **English**, depending on the language of the question. Do NOT translate. Reply in the same language.\n",
    "\n",
    "Use the following context from official documents to answer the user's question. \n",
    "\n",
    "--- CONTEXT START ---\n",
    "{context}\n",
    "--- CONTEXT END ---\n",
    "\n",
    "--- USER QUESTION ---\n",
    "{query}\n",
    "--- END ---\n",
    "\n",
    "Guidelines:\n",
    "- ‚úÖ Respond only using the given context.\n",
    "- ‚ùå If not found in context, say \"I'm not sure based on the available information.\"\n",
    "- üßæ Format the answer in **clear bullet points or numbered steps**.\n",
    "- üîÅ Do not repeat the question.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "This is a **structured, multilingual-aware prompt** that:\n",
    "\n",
    "* Tells Gemini it is a **Sri Lankan government services assistant**.\n",
    "* Forces the answer to be in **the same language as the query**.\n",
    "* Restricts answers to **only the retrieved context**.\n",
    "* Encourages **bullet points or numbered steps**.\n",
    "* Tells it **not to repeat the question**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Generate the Answer**\n",
    "\n",
    "```python\n",
    "try:\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "except Exception as e:  \n",
    "    return f\"Gemini API error: {e}\"\n",
    "```\n",
    "\n",
    "* **`generate_content`** ‚Üí Sends the prompt to Gemini and gets a response.\n",
    "* **`.text.strip()`** ‚Üí Returns the clean text answer (without extra spaces).\n",
    "* If Gemini API fails ‚Üí Returns an error message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658bb04",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:black; color:lime; padding:10px; font-family:monospace;\">\n",
    "üìú app.py\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad3323",
   "metadata": {},
   "source": [
    "## **üìÇ File Overview**\n",
    "\n",
    "This script is a **Streamlit web app** that acts as a **chatbot interface** for answering questions about Sri Lankan government services.\n",
    "It connects the **frontend UI** (Streamlit) with your **backend logic** (`answer_query_with_gemini` function) to display responses in a **chat format**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Imports**\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "from modules.qa_with_retriever import answer_query_with_gemini\n",
    "```\n",
    "\n",
    "* **`streamlit` (`st`)** ‚Üí Used to create the **UI components** (sidebar, inputs, chat bubbles).\n",
    "* **`answer_query_with_gemini`** ‚Üí The **function you wrote earlier** that:\n",
    "\n",
    "  1. Retrieves relevant documents from ChromaDB.\n",
    "  2. Sends them to **Google Gemini** for an AI-generated answer.\n",
    "  3. Returns that answer back to the app.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Page Configuration**\n",
    "\n",
    "```python\n",
    "st.set_page_config(\n",
    "    page_title=\"üá±üá∞ Gov Services FAQ Bot\",\n",
    "    page_icon=\"ü§ñ\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "```\n",
    "\n",
    "* **`page_title`** ‚Üí Title shown in the browser tab.\n",
    "* **`page_icon`** ‚Üí Emoji for the favicon.\n",
    "* **`layout=\"wide\"`** ‚Üí Makes the app span the entire width.\n",
    "* **`initial_sidebar_state=\"expanded\"`** ‚Üí Sidebar is open by default.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Sidebar UI**\n",
    "\n",
    "```python\n",
    "with st.sidebar:\n",
    "    st.title(\"ü§ñ Sri Lanka Gov FAQ Bot\")\n",
    "    st.markdown(\"\"\" ... \"\"\")\n",
    "    st.caption(\"Powered with ‚ù§Ô∏è by Gemini, Cohere & Streamlit\")\n",
    "```\n",
    "\n",
    "* Displays **bot name** and description.\n",
    "* Lists the **technologies used**.\n",
    "* Gives **instructions** on how to use the bot.\n",
    "* Adds a **developer credit section** (name, GitHub, email).\n",
    "* Includes a **disclaimer** (important for AI apps).\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Main Title**\n",
    "\n",
    "```python\n",
    "st.markdown(\n",
    "    \"<h2 style='text-align:center; margin-bottom:0;'>üá±üá∞ Government Services FAQ Assistant</h2>\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "st.markdown(\n",
    "    \"<p style='text-align:center; color: gray; margin-top:5px;'>Ask questions in Tamil, Sinhala, or English</p>\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "```\n",
    "\n",
    "* HTML is used here to **center-align** and **style** the text.\n",
    "* `unsafe_allow_html=True` ‚Üí Allows HTML styling inside Streamlit markdown.\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ Chat History Storage**\n",
    "\n",
    "```python\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "```\n",
    "\n",
    "* `st.session_state` ‚Üí Stores **persistent data** between user interactions.\n",
    "* `\"messages\"` ‚Üí List of all past messages (`role` and `content`) so that:\n",
    "\n",
    "  * Chat history **doesn‚Äôt disappear** after every response.\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ Display Previous Chat Messages**\n",
    "\n",
    "```python\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.markdown(msg[\"content\"], unsafe_allow_html=True)\n",
    "```\n",
    "\n",
    "* Loops through `messages` and displays each in a **chat bubble**.\n",
    "* `msg[\"role\"]` is either `\"user\"` or `\"assistant\"`.\n",
    "* `msg[\"content\"]` is the actual text.\n",
    "\n",
    "---\n",
    "\n",
    "## **7Ô∏è‚É£ User Input Field**\n",
    "\n",
    "```python\n",
    "query = st.chat_input(\"Type your question about passports, NIC, land docs, and more...\")\n",
    "```\n",
    "\n",
    "* Special Streamlit component for a **chat-style input box**.\n",
    "* When the user types and hits **Enter**, `query` will store that text.\n",
    "\n",
    "---\n",
    "\n",
    "## **8Ô∏è‚É£ When User Sends a Message**\n",
    "\n",
    "```python\n",
    "if query:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "```\n",
    "\n",
    "* Adds the user‚Äôs question to chat history.\n",
    "\n",
    "```python\n",
    "with st.chat_message(\"user\"):\n",
    "    st.markdown(query)\n",
    "```\n",
    "\n",
    "* Displays the user‚Äôs message immediately.\n",
    "\n",
    "---\n",
    "\n",
    "## **9Ô∏è‚É£ Getting the Bot‚Äôs Answer**\n",
    "\n",
    "```python\n",
    "with st.chat_message(\"assistant\"):\n",
    "    with st.spinner(\"Getting answer...\"):\n",
    "        result = answer_query_with_gemini(query)\n",
    "```\n",
    "\n",
    "* Shows a **loading spinner** while fetching the answer.\n",
    "* Calls **`answer_query_with_gemini`** to:\n",
    "\n",
    "  * Retrieve relevant data.\n",
    "  * Ask Gemini to answer.\n",
    "  * Return the final reply.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîü Processing the Response**\n",
    "\n",
    "```python\n",
    "if isinstance(result, dict):\n",
    "    answer = result.get(\"answer\", \"\")\n",
    "    sources = result.get(\"sources\", [])\n",
    "else:\n",
    "    answer = result\n",
    "    sources = []\n",
    "```\n",
    "\n",
    "* Handles both:\n",
    "\n",
    "  * **Dict format**: Contains `\"answer\"` and `\"sources\"`.\n",
    "  * **String format**: Just an answer without sources.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£1Ô∏è‚É£ Formatting & Display**\n",
    "\n",
    "```python\n",
    "formatted_answer = answer.replace(\"\\n\", \"<br>\")\n",
    "st.markdown(formatted_answer, unsafe_allow_html=True)\n",
    "```\n",
    "\n",
    "* Replaces line breaks (`\\n`) with `<br>` so they render properly in HTML.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£2Ô∏è‚É£ Showing Sources**\n",
    "\n",
    "```python\n",
    "if sources:\n",
    "    st.markdown(\"<hr>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"**Sources:**\")\n",
    "    for s in sources:\n",
    "        st.markdown(f\"- `{s}`\")\n",
    "```\n",
    "\n",
    "* If sources are available, they‚Äôre displayed below the answer.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£3Ô∏è‚É£ Saving Bot‚Äôs Message to History**\n",
    "\n",
    "```python\n",
    "st.session_state.messages.append({\"role\": \"assistant\", \"content\": formatted_answer})\n",
    "```\n",
    "\n",
    "* Stores the assistant‚Äôs reply so it‚Äôs visible in future renders.\n",
    "\n",
    "---\n",
    "\n",
    "## **üí° How It Works in Sequence**\n",
    "\n",
    "1. User visits the page ‚Üí sees the **sidebar instructions** and **title**.\n",
    "2. User types a question in the chat box.\n",
    "3. Question is stored in session state and displayed.\n",
    "4. The app calls `answer_query_with_gemini(query)`:\n",
    "\n",
    "   * Retrieves relevant documents from Chroma.\n",
    "   * Sends them to Gemini AI for processing.\n",
    "5. Gemini returns an answer.\n",
    "6. Answer is **formatted** and displayed with any sources.\n",
    "7. Both question and answer are **saved to session state** for history.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56e3a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
