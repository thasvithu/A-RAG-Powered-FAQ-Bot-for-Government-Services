import os
import google.generativeai as genai
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()


# Load Gemini API key from environment
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
gemini_model = genai.GenerativeModel("gemini-1.5-flash")

# Set up vectorstore and retriever once
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
persist_directory = "./chroma_store"
vectorstore = Chroma(
    persist_directory=persist_directory,
    embedding_function=embedding_model
)
retriever = vectorstore.as_retriever()

# Function to handle RAG with Gemini
def answer_query_with_gemini(query: str) -> str:
    """
    Retrieve relevant documents from Chroma and generate answer using Gemini.

    Args:
        query (str): The user's question.

    Returns:
        str: The response generated by Gemini.
    """
    # Step 1: Get relevant documents
    docs = retriever.get_relevant_documents(query)
    if not docs:
        return "No relevant documents found for your query."

    # Step 2: Combine docs into a single string
    retrieved_text = "\n\n".join([doc.page_content for doc in docs])

    # Step 3: Prepare input for Gemini
    prompt = f"""Use the following information to answer the user's question.

    Context:
    {retrieved_text}

    Question:
    {query}
    """

    try:
        response = gemini_model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"Gemini API error: {e}"
