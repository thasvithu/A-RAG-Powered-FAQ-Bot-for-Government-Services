<div align="center">

# ğŸ‡±ğŸ‡° RAGâ€‘Powered Government Services FAQ Bot

Multilingual (Sinhala | Tamil | English) Retrievalâ€‘Augmented Generation chatbot that answers citizen questions about Sri Lankan government services using official PDF documents.

</div>

---

## ğŸš€ Overview
This project implements a Retrievalâ€‘Augmented Generation (RAG) pipeline: it ingests official PDFs, chunks & embeds them with Cohere multilingual embeddings, stores vectors in ChromaDB, retrieves the most relevant passages for a user question, and uses Google Gemini to generate grounded, bulletâ€‘point answers in the same language as the query.

## âœ¨ Key Features
- Multilingual query + answer (Sinhala / Tamil / English) â€” responds in the query language
- Grounded answers: refuses to hallucinate outside provided context
- Persistent vector store (Chroma) â€” fast restarts without reâ€‘embedding
- Streamlit chat UI with history
- Modular pipeline (loader â†’ splitter â†’ embed/store â†’ retriever â†’ LLM)

## ğŸ§± Tech Stack
- LangChain (documents, splitting, orchestration)
- Cohere Multilingual Embeddings (`embed-multilingual-v3.0`)
- ChromaDB (vector storage)
- Google Gemini 1.5 Flash (generation)
- Streamlit (UI)
- PyPDF (`pypdf` via `PyPDFLoader`)

## ğŸ“‚ Project Structure
```
app.py                     # Streamlit chat application
main.py                    # Simple ingestion + demo query script
modules/
	pdf_loader.py           # PDF -> Documents
	text_splitter.py        # Recursive chunking
	embed_store.py          # Embedding + persistence
	retriever.py            # Retriever factory
	qa_with_retriever.py    # Retrieval + Gemini answer
data/                     # Source PDFs (ingestion corpus)
chroma_store/             # Persisted Chroma vector DB
notebooks/                # Experiments & workflow exploration
requirements.txt          # Python dependencies (pin / install)
```

## ğŸ” Environment Variables
Create a `.env` file in the project root:
```
COHERE_API_KEY=your_cohere_key
GOOGLE_API_KEY=your_gemini_key
```

## ğŸ› ï¸ Setup
```powershell
# (Optional) Create & activate virtual environment
python -m venv .venv
./.venv/Scripts/Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“¥ Ingest Documents
Place PDF files in `data/` then run:
```powershell
python main.py
```
This will:
1. Load the sample `Passport1.pdf`
2. Split into chunks
3. Embed & persist to `./chroma_store`
4. Run a sample question

Reâ€‘run only when adding or updating source PDFs. (A future improvement could add duplicate chunk detection.)

## ğŸ’¬ Run the Chat App
```powershell
streamlit run app.py
```
Open the provided local URL. Ask questions such as:
- Sinhala: à·à·Šâ€à¶»à·“ à¶½à¶‚à¶šà·à·€à·š à¶´à·à·ƒà·Šà¶´à·à¶§à·Š à¶½à¶¶à·à¶œà·à¶±à·“à¶¸à·š à¶šà·Šâ€à¶»à·’à¶ºà·à·€à¶½à·’à¶º à¶¸à·œà¶šà¶šà·Šà¶¯?
- Tamil: à®‡à®²à®™à¯à®•à¯ˆà®¯à®¿à®²à¯ à®ªà®¾à®¸à¯à®ªà¯‹à®°à¯à®Ÿà¯ à®ªà¯†à®± à®à®¨à¯à®¤ à®ªà®Ÿà®¿à®•à®³à¯?
- English: What documents are needed for a Sri Lankan passport?

## âš™ï¸ Configuration (current defaults)
| Aspect | Location | Default | Notes |
|--------|----------|---------|-------|
| Chunk size | `text_splitter.py` | 512 | Increase for fewer, larger chunks |
| Chunk overlap | `text_splitter.py` | 64 | Maintain semantic continuity |
| Embedding model | `embed_store.py` / `retriever.py` | embed-multilingual-v3.0 | Cohere multilingual |
| Retriever k | `retriever.py` | 5 | Number of context chunks sent to Gemini |
| Vector store dir | Code args | ./chroma_store | Persist across sessions |

To adjust, edit the corresponding module or refactor into a config file (see Roadmap).

## ğŸ§ª Testing Ideas (Not Yet Implemented)
Planned lightweight checks:
- Ingestion smoke test: ensure â‰¥1 chunk, embeddings count matches
- Retrieval test: known query returns at least one relevant chunk
- Prompt safety test: injection attempt returns refusal phrase

## ğŸ”„ RAG Flow
1. Load PDFs â†’ LangChain Documents (page metadata)
2. Split into overlapping chunks (recursive splitter)
3. Embed chunks (Cohere) â†’ store vectors (Chroma persistent)
4. User asks a question â†’ retrieve top-k similar chunks
5. Construct grounded multilingual prompt â†’ Gemini Flash
6. Return bulletâ€‘point answer (no hallucinated extras)

## ğŸ—ºï¸ Roadmap / Next Steps
- [ ] Add metadata (page, source) to answers as citations
- [ ] Structured response `{ answer, sources[] }` always
- [ ] Config file (YAML or `.env` expansions) for tunables
- [ ] Duplicate chunk hash + skip reâ€‘embedding
- [ ] Reranker (Cohere Rerank or crossâ€‘encoder) postâ€‘retrieval
- [ ] Evaluation notebook (Recall@K, groundedness LLM judge)
- [ ] Streaming responses in UI
- [ ] Upload new PDF via UI + background embedding job
- [ ] Semantic cache for repeated queries
- [ ] Improved prompt injection mitigation

## âš ï¸ Disclaimer
This assistant provides informational responses based on supplied documents and is **not** an official government service. Always verify critical requirements with official sources.

## ğŸ“„ License
See `LICENSE` for details.

## ğŸ™Œ Acknowledgements
- Cohere & Google for API access
- LangChain & Chroma communities

---

Feel free to open issues or PRs to improve ingestion, evaluation, or multilingual robustness.